python hyde_fc_generation_optimized.py --target_data "data_store/averitec/dev.json" --json_output "data_store/dev_hyde_fc_llama3_1_optimized.json"

Timing Statistics:
Total Runtime: 0:42:55
Average Batch Time: 159.23 seconds
Average Time per Example: 5.15 seconds
Throughput: 0.19 examples/second

Saving results...
Script completed at: 2024-12-11 13:30:08
Total runtime: 0:42:55


python retrieval_parallel.py --knowledge_store_dir "knowledge_store/dev" --target_data "data_store/dev_hyde_fc_llama3_1_optimized.json" --json_output "data_store/dev_retrieval_top_k_optimized_check.json" --top_k 10000

Timing Summary:
Start time: 2025-01-07 09:49:59
End time: 2025-01-07 10:01:51
Total runtime: 0:11:51 (HH:MM:SS)
Average time per example: 1.42 seconds
Processing speed: 0.70 examples per second


python reranking_optimized.py --target_data "data_store/dev_retrieval_top_k_optimized_check.json" --json_output "data_store/dev_reranking_top_k_optimized_check.json" --retrieved_top_k 500

Start time: 2025-01-07 13:23:13
End time: 2025-01-07 19:13:51
Total runtime: 5:50:38 (HH:MM:SS)
Average time per example: 42.08 seconds
Processing speed: 0.02 examples per second

python question_generation_optimized.py --reference_corpus "data_store/averitec/dev.json" --top_k_target_knowledge "data_store/dev_reranking_top_k_optimized_check.json" --output_questions "data_store/dev_top_k_qa_optimized_check.json" --model "meta-llama/Meta-Llama-3-8B-Instruct"

Timing Summary:
Start time: 2025-01-07 19:44:34
End time: 2025-01-07 20:19:18
Total runtime: 0:34:44
Setup time: 0:00:34
Average time per example: 3.8 seconds
Processing time: 0:34:10

python veracity_prediction_optimized.py --target_data "data_store/dev_top_k_qa_optimized_check.json" --output_file "data_store/dev_veracity_prediction_optimized_check.json" --model "humane-lab/Meta-Llama-3.1-8B-HerO"

Timing Summary:
Start time: 2025-01-07 20:42:46
End time: 2025-01-07 20:49:36
Total runtime: 0:06:50
Data loading time: 0:00:00
Model initialization time: 0:02:52
Processing time: 0:03:58
Average time per example: 0.48s
Processing speed: 2.10 examples per second


Question-only score (HU-meteor):             0.5472870417630183
Question-answer score (HU-meteor):           0.3637677041460875
====================
Veracity F1 scores:
 * Supported:                                0.6245059288537549
 * Refuted:                                  0.8088467614533965
 * Not Enough Evidence:                      0.12244897959183673
 * Conflicting Evidence/Cherrypicking:       0.18461538461538463
 * macro:                                    0.43510426362859317
 * acc:                                      0.688
--------------------
AVeriTeC scores:
 * Veracity scores (meteor @ 0.1):           0.688
 * Veracity scores (meteor @ 0.2):           0.622
 * Veracity scores (meteor @ 0.25):          0.522
 * Veracity scores (meteor @ 0.3):           0.442
 * Veracity scores (meteor @ 0.4):           0.23
 * Veracity scores (meteor @ 0.5):           0.134
--------------------
AVeriTeC scores by type @ 0.25:
 * Veracity scores (Event/Property Claim):   0.3351794497121063
 * Veracity scores (Position Statement):     0.32700024082698753
 * Veracity scores (Causal Claim):           0.25440381764017306
 * Veracity scores (Numerical Claim):        0.3229371275549023
 * Veracity scores (Quote Verification):     0.28713365192570844