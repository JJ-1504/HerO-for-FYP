python hyde_fc_generation_optimized.py --target_data "data_store/averitec/dev.json" --json_output "data_store/dev_hyde_fc_llama3_1_optimized.json"

Timing Statistics:
Total Runtime: 0:42:55
Average Batch Time: 159.23 seconds
Average Time per Example: 5.15 seconds
Throughput: 0.19 examples/second

Saving results...
Script completed at: 2024-12-11 13:30:08
Total runtime: 0:42:55


python retrieval_parallel.py --knowledge_store_dir "knowledge_store/dev" --target_data "data_store/dev_hyde_fc_llama3_1_optimized.json" --json_output "data_store/dev_retrieval_top_k_optimized.json"

Timing Summary:
Start time: 2024-12-11 15:45:32
End time: 2024-12-12 06:09:40
Total runtime: 14:24:08 (HH:MM:SS)
Average time per example: 103.70 seconds
Processing speed: 0.01 examples per second


python reranking.py --target_data "data_store/dev_retrieval_top_k_optimized.json" --json_output "data_store/dev_reranking_top_k_optimized.json" --retrieved_top_k 500

Timing Summary:
Start time: 2024-12-12 19:39:07
End time: 2024-12-13 01:17:52
Total runtime: 5:38:45 (HH:MM:SS)
Average time per example: 40.65 seconds
Processing speed: 0.02 examples per second


python question_generation_optimized.py --reference_corpus "data_store/averitec/dev.json" --top_k_target_knowledge "data_store/dev_reranking_top_k_optimized.json" --output_questions "data_store/dev_top_k_qa_optimized.json" --model "meta-llama/Meta-Llama-3-8B-Instruct"

Timing Summary:
Start time: 2024-12-13 08:06:34
End time: 2024-12-13 08:41:45
Total runtime: 0:35:11
Setup time: 0:00:31
Processing time: 0:34:40
Average time per example: 3.9 seconds
Results written to: data_store/dev_top_k_qa_optimized.json


python veracity_prediction_optimized.py --target_data "data_store/dev_top_k_qa_optmized.json" --output_file "data_store/dev_veracity_prediction.json" --model "humane-lab/Meta-Llama-3.1-8B-HerO"


Timing Summary:
Start time: 2024-12-13 08:50:49
End time: 2024-12-13 08:59:12
Total runtime: 0:08:23
Data loading time: 0:00:00
Model initialization time: 0:04:48
Processing time: 0:03:36
Average time per example: 0.43s
Processing speed: 2.32 examples per second




python averitec_evaluate.py --prediction_file "data_store/dev_veracity_prediction_optimized.json" --label_file "data_store/averitec/dev.json"

Question-only score (HU-meteor):             0.5379427223750081
Question-answer score (HU-meteor):           0.35476456251812
====================
Veracity F1 scores:
 * Supported:                                0.6715867158671587
 * Refuted:                                  0.8411214953271028
 * Not Enough Evidence:                      0.15384615384615385
 * Conflicting Evidence/Cherrypicking:       0.16666666666666666
 * macro:                                    0.4583052579267705
 * acc:                                      0.736
--------------------
AVeriTeC scores:
 * Veracity scores (meteor @ 0.1):           0.734
 * Veracity scores (meteor @ 0.2):           0.684
 * Veracity scores (meteor @ 0.25):          0.57
 * Veracity scores (meteor @ 0.3):           0.44
 * Veracity scores (meteor @ 0.4):           0.228
 * Veracity scores (meteor @ 0.5):           0.112
--------------------
AVeriTeC scores by type @ 0.25:
 * Veracity scores (Event/Property Claim):   0.3195212746696627
 * Veracity scores (Position Statement):     0.3276382139221968
 * Veracity scores (Causal Claim):           0.2649202261906687
 * Veracity scores (Numerical Claim):        0.3145378111766672
 * Veracity scores (Quote Verification):     0.2879181948896812


Overall average per sample: 5.15 + 103.7 +40.65 + 3.90 + 0.43  = ca 150sec -> 2.5 Minutes
